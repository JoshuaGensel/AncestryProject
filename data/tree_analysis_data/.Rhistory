x[1]-mean(x)/sd(x)
x <- (x-mean(x))/sd(x)
x[1]
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
x <- (x-mean(x))/sd(x)
x[1]
(x[1]-mean(x))/sd(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
summary(lm(y ~x))
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
library(swirl)
swirl()
fit <- lm(child ~ parent, galton)
sqrt(sum(fit$residuals^2/(n-2)))
sqrt(sum(fit$residuals^2) / (n - 2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
mu <- mean(galton$child)
sTot <- sum((galton$child-mu)^2)
sRes <- deviance(fit)
1- sRes/sTot
summary(fit)$r.squared
cor(galton$child,galton$parent)^2
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y ~ x)
summary(fit)
library(datasets)
dat(mtcars)
data(mtcars)
fit2 <- lm(mpg ~ wt, mtcars)
View(mtcars)
sumCoef <- summary(fit2)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]
sumCoef
predict(fit2, newdata = mean(mtcars$wt), interval = "confidence")
predict(fit2, newdata = data.frame(x = mean(mtcars$wt)), interval = "confidence")
View(mtcars)
predict(fit2, newdata = data.frame(wt = mean(mtcars$wt)), interval = "confidence")
?mtcars
predict(fit2, newdata = data.frame(wt = 3), interval = "prediction")
sumCoef <- summary(fit2)$coefficients
sumCoef
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]
(sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2])*2
(sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2])/2
sumCoef <- summary(fit2)$coefficients
sumCoef
sumCoef[2]
(sumCoef[2,2] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2])/2
(sumCoef[2] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2])/2
(sumCoef[2] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2])/2
sumCoef[1,1]
sumCoef[1,2]
sumCoef[2,1]
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2])/2
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2])
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2])*2
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2])/2
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2])
sumCoef[2,2]
sumCoef[2,1]
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2])
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2])/2
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2])*2
summary(fit2)
swirl()
library(swirl)
swirl()
rgp1()
rgp2()
head(swiss)
mdl <- lm(Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality, swiss)
vif(mdl)
mdl2 <- lm(Fertility ~ Agriculture + Education + Catholic + Infant.Mortality, swiss)
vif(mdl2)
x1c <- simbias()
apply(x1c, 1, mean)
fit1 <- lm(Fertility ~ Agriculture)
fit1 <- lm(Fertility ~ Agriculture, swiss)
fit3 <- lm(Fertility ~ Agriculture + Examination + Education, swiss)
anova(fit1, fit3)
deviance(fit3)
2
d <- deviance(fit3)/43
2
n <- (deviance(fit1) - deviance(fit3))/2
n/d
pf(n/d, 2, 43, lower.tail=FALSE)
shapiro.test(fit3$residuals)
anova(fit1, fit3, fit5, fit6)
ravenData
2
mdl <- glm(ravenWinNum ~ ravenScore, binomial, ravenData)
lodds <- predict(mdl, data.frame(ravenScore=c(0, 3, 6)))
exp(lodds)/(1+exp(lodds))
summary(mdl)
exp(confint(mdl))
anova(mdl)
qchisq(0.95, 1)
var(rpois(1000, 50))
nxt()
head(hits)
class(hits[,'date'])
as.integer(head(hits[,'date']))
mdl <- glm(visits ~ date, poisson, hits)
summary(mdl)
exp(confint(mdl, 'date'))
which.max(hits[,'visits'])
hits[704,]
mdl$fitted.values[704]
lambda <- mdl$fitted.values[704]
qpois(.95, lambda)
2
mdl2 <- glm(formula = simplystats ~ date, family = poisson, data = hits, offset = log(visits + 1))
2
qpois(.95, mdl2$fitted.values[704])
library(MASS)
data("shuttle")
force(shuttle)
View(shuttle)
?shuttle
library(tidyverse)
shuttle <- shuttle %>% mutate(auto = case_when( use == "auto" ~ 1, use == "noauto" ~ 0))
View(shuttle)
?glm
fit1 <- glm(auto ~ wind, binomial)
fit1 <- glm(auto ~ wind, binomial,shuttle)
summary(fit1)
exp(fit1$coefficients[2])
View(shuttle)
fit2 <- glm(auto ~ wind + magn, binomial,shuttle)
summary(fit2)
fit2 <- glm(auto ~ magn + wind, binomial,shuttle)
summary(fit2)
exp(fit1$coefficients)
exp(fit2$coefficients)
fit3 <- glm(I(1-auto ~ wind, binomial, shuttle))
fit3 <- glm(I(1-auto) ~ wind, binomial, shuttle)
summary(fit3)
summary(fit1)
data("InsectSprays")
force(InsectSprays)
glm(count ~ spray, poisson, InsectSprays)
fit4 <- glm(count ~ spray, poisson, InsectSprays)
summary(fit4)
exp(fit4$coefficients)
14.5/(14.5+1.0574713)
14.5/(14.5+1.05)
fit5 <- glm(count ~ spray + offset(2), poisson, InsectSprays)
fit5 <- glm(count ~ spray + offset(1), poisson, InsectSprays)
fit5 <- glm(count ~ spray + offset(log(1)), poisson, InsectSprays)
fit5 <- glm(count ~ spray + offset(log(2)), poisson, InsectSprays)
fit5 <- glm(count ~ spray + offset(log(1)), poisson, InsectSprays)
data("Seatbelts")
fit5 <- glm(DriversKilled ~ pp + mmc + law + offset(log(drivers)), poisson, Seatbelts)
data("Seatbelts")
force(Seatbelts)
data.frame(Seatbelts)
seatbelts <- data.frame(Seatbelts)
View(seatbelts)
fit5 <- glm(DriversKilled ~ kms + law + offset(log(drivers)), poisson, Seatbelts)
fit5 <- glm(DriversKilled ~ kms + law + offset(log(drivers)), poisson, seatbelts)
summary(fit5)
fit6 <- glm(DriversKilled ~ kms + law + offset(log(10)+log(drivers)), poisson, seatbelts)
summary(fit6)
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knot = 0
knot <- 0
splineTerm <- (x>knot)*(x-knot)
splineTerms <- (x>knot)*(x-knot)
xMat <- cbind(1, x, splineTerms)
yhat <- predict(lm(y ~ xMat - 1))
plot(x, y, frame = FALSE, pch = 21, bg = "lightblue", cex = 2)
lines(x, yhat, col = "red", lwd = 2)
summary(lm(y ~ xMat - 1))
exp(fit1$coefficients)
summary(fit1)
exp(fit1$coefficients)
fit1$coefficients
fit1$coefficients[1]
fit1$coefficients[1]/fit1$coefficients[2]
fit1$coefficients
0.25131443/0.03181183
exp(fit1$coefficients[1])/exp(fit1$coefficients[2])
exp(fit1$coefficients[1])/exp(fit1$coefficients[2])
exp(0.25131443)/exp(0.03181183)
exp(fit2$coefficients)/exp(fit2$coefficients)
summary(fit1)
exp(coef(fit1))
fit1 <- glm(auto ~ I(1 * (wind == "head")), binomial,shuttle)
exp(coef(fit1))
fit2 <- glm(auto ~ magn + I(1 * (wind == "head")), binomial,shuttle)
exp(fit2$coefficients)
install.packages("caret", dependencies = c("Depends", "Suggests"))
library(caret)
library(caret)
library(caret)
install.packages("caret")
library(caret)
install.packages("rlang")
install.packages("rlang")
library(rlang)
sessionInfo()
install.packages("caret")
library(caret)
install.packages("caret", dependencies = c("Depends", "Suggests"))
library(caret)
install.packages("caret", dependencies=c("Depends", "Imports"))
library(caret)
sessionInfo()
library(installr)
sessionInfo()
library(installr)
updateR()
install.packages("caret", dependencies=c("Depends", "Imports"))
library(caret)
install.packages("gower")
library(caret)
install.packages("hardhat")
library(caret)
library(tidyverse)
library(ggplot2)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(train())
head(train
)
head(training)
qplot(CompressiveStrength, Index, training)
qplot(CompressiveStrength, Index, training, geom = "point")
install.packages("Hmisc")
?cut2
library(Hmisc)
?cut2
ggplot(training, aes(x=nrow(),y=CompressiveStrength))
plot(training$CompressiveStrength)
head(training)
ggplot(training, aes(y = CompressiveStrength, col = FlyAsh))
ggplot(training, aes(y = CompressiveStrength, col = FlyAsh)) + geom_point()
ggplot(training, aes(x= row.name(), y = CompressiveStrength, col = FlyAsh)) + geom_point()
training$index = as.numeric(row.names(training))
View(training)
ggplot(training, aes(x = index, y = CompressiveStrength, col = FlyAsh)) + geom_point()
ggplot(training, aes(x = index, y = CompressiveStrength, col = Age)) + geom_point()
ggplot(training, aes(x = index, y = CompressiveStrength, col = FlyAsh)) + geom_point()
hist(training$Superplasticizer)
hist(training$Superplasticizer)
hist(log(training$Superplasticizer))
hist(training$Superplasticizer)
hist(log(training$Superplasticizer+1))
hist(training$Superplasticizer)
hist(log(training$Superplasticizer+1))
hist(training$Superplasticizer)
hist(log(training$Superplasticizer+1))
hist(training$Superplasticizer)
hist(log(training$Superplasticizer+1))
hist(training$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(training)
ILpredictors <- select(training, starts_with("IL"))
colnames(ILpredictors)
?preProcess
preProc <- preProcess(ILpredictors, method = 'pca')
View(preProc)
View(preProc)
summary(preProc)
pca <- prcomp(ILpredictors, scale. = TRUE)
summary(pca)
summary(pca)[1]
summary(pca)[2]
summary(pca)[1]
summary(pca)[1,]
summary(pca)[1]
summary(pca)$prop
summary(pca)$p
summary(pca)[3]
summary(pca)[4]
summary(pca)[5]
summary(pca)[6]
summary(pca)[7]
summary(pca)[6][1]
summary(pca)[6][2]
summary(pca)
summary(pca)[2,]
summary(pca)$importance[2,]
varianceexplained = cumsum(summary(pca)$importance[2,])
varianceexplained
colnames(training)
ILpredictors <- select(training, starts_with("IL"), diagnosis)
colnames(ILpredictors)
ILpredictors <- select(training,  diagnosis, starts_with("IL"))
colnames(ILpredictors)
?train
allFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm")
ILpredictors <- select(training,  diagnosis, starts_with("IL"))
preProc <- preProcess(ILpredictors,method="pca",pcaComp=7)
allFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm")
pcaFit <- allFit <- train(preProc, method = "glm")
pcaFit <- allFit <- train(preProc,ILpredictors, method = "glm")
pcaFit <- train(preProc, method = "glm")
pcaFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm", preProcess= "pca")
allFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm")
pcaFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm", preProcess= "pca")
confusionMatrix(allFit)
confusionMatrix(pcaFit)
preProc <- preProcess(ILpredictors,method="pca",pcaComp=7)
allFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm")
pcaFit <- train(diagnosis ~ . ,data = preProc, method = "glm")
allFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm")
pcaFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm")
confusionMatrix(allFit)
confusionMatrix(pcaFit)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ILpredictors <- select(training,  diagnosis, starts_with("IL"))
allFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm")
pcaFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm")
confusionMatrix(allFit)
confusionMatrix(pcaFit)
confusionMatrix(testing$diagnosis, predict(allFit,diagnosis))
confusionMatrix(testing$diagnosis, predict(allFit,diagnosis))
confusionMatrix(testing$diagnosis, predict(pcaFit,diagnosis))
"IL_11" %in% colnames(testing)
"IL_11" %in% colnames(ILpredictors)
allFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm")
pcaFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm")
confusionMatrix(testing$diagnosis, predict(allFit,diagnosis))
confusionMatrix(testing$diagnosis, predict(pcaFit,diagnosis))
confusionMatrix(testing$diagnosis, predict(allFit,testing))
confusionMatrix(testing$diagnosis, predict(pcaFit,testing))
allFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm")
pcaFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm", preProcess = "pca")
confusionMatrix(testing$diagnosis, predict(allFit,testing))
confusionMatrix(testing$diagnosis, predict(pcaFit,testing))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ILpredictors <- select(training,  diagnosis, starts_with("IL"))
preProc <- preProcess(ILpredictors,method="pca",pcaComp=7)
allFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm")
pcaFit <- train(diagnosis ~ . ,data = ILpredictors, method = "glm", preProcess = "pca")
confusionMatrix(testing$diagnosis, predict(allFit,testing))
confusionMatrix(testing$diagnosis, predict(pcaFit,testing))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ILpredictors <- select(training, starts_with("IL"))
colnames(ILpredictors)
pca <- prcomp(ILpredictors, scale. = TRUE)
summary(pca)$importance[2,]
varianceexplained = cumsum(summary(pca)$importance[2,])
varianceexplained
1-(2/3)^2-(1/3)^2
1-(2/3)^2-(2/3)^2
1-(2/4)^2-(2/4)^2
1-(4/4)^2-(0/4)^2
################################################################################
#Data handling
library(tidyverse)
library(ggplot2)
require(GGally)
require(rgl)
require(betareg)
setwd("D:/Daten/programming_projects/AncestryProject/data/tree_analysis_data")
treedata_Y <- read.csv("treedata_Y.csv", header = FALSE)
treedata_M <- read.csv("treedata_M.csv", header = FALSE)
treedata <- rbind(treedata_M, treedata_Y)
colnames(treedata) <- strsplit(read_lines("headers.txt"), split = ",")[[1]]
View(treedata)
treedata = treedata %>%
mutate(G_DIFF = sqrt((TRUE_P1-G_P1)^2))
################################################################################
#Exploratory Analysis
cor(treedata, use = "pairwise.complete")
summary(treedata$G_DIFF)
ggplot(treedata, aes(x = TD, y = G_NO_INFO, col = TA)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TD, y = G_UNKNOWN, col = TA)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TA, y = G_FALSE, col = TD)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TA, y = G_DIFF, col = TD)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TA, y = G_FALSE, col = TD)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TA, y = G_DIFF, col = TD)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TA, y = sqrt((INIT_P1-G_P1)^2), col = TD)) +
facet_grid(~NS) +
geom_point()
plot3d(treedata$TD,treedata$TA,treedata$G_DIFF)
################################################################################
#Data handling
library(tidyverse)
library(ggplot2)
require(GGally)
require(rgl)
require(betareg)
setwd("D:/Daten/programming_projects/AncestryProject/data/tree_analysis_data")
treedata_Y <- read.csv("treedata_Y.csv", header = FALSE)
treedata_M <- read.csv("treedata_M.csv", header = FALSE)
treedata <- rbind(treedata_M, treedata_Y)
colnames(treedata) <- strsplit(read_lines("headers.txt"), split = ",")[[1]]
treedata = treedata %>%
mutate(G_DIFF = sqrt((TRUE_P1-G_P1)^2))
summary(treedata$G_DIFF)
ggplot(treedata, aes(x = TA, y = G_FALSE, col = TD)) +
geom_point()
ggplot(treedata, aes(x = TA, y = G_DIFF, col = TD)) +
geom_point()
################################################################################
#Data handling
library(tidyverse)
library(ggplot2)
require(betareg)
setwd("D:/Daten/programming_projects/AncestryProject/data/tree_analysis_data")
treedata_Y <- read.csv("treedata_Y.csv", header = FALSE)
treedata_M <- read.csv("treedata_M.csv", header = FALSE)
treedata <- rbind(treedata_M, treedata_Y)
colnames(treedata) <- strsplit(read_lines("headers.txt"), split = ",")[[1]]
View(treedata)
treedata = treedata %>%
mutate(G_DIFF = sqrt((TRUE_P1-G_P1)^2))
################################################################################
#Exploratory Analysis
cor(treedata, use = "pairwise.complete")
summary(treedata$G_DIFF)
ggplot(treedata, aes(x = TD, y = G_NO_INFO, col = TA)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TD, y = G_UNKNOWN, col = TA)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TA, y = G_FALSE, col = TD)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TD, y = G_FALSE, col = TA)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TA, y = G_DIFF, col = TD)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TD, y = G_DIFF, col = TA)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TA, y = sqrt((INIT_P1-G_P1)^2), col = TD)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TD, y = sqrt((INIT_P1-G_P1)^2), col = TA)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TD, y = G_UNKNOWN, col = TA)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TD, y = G_UNKNOWN/NS, col = TA)) +
facet_grid(~NS) +
geom_point()
ggplot(treedata, aes(x = TD, y = G_FALSE/NS, col = TA)) +
facet_grid(~NS) +
geom_point()
